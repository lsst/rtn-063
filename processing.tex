\section{PDR2 processing on USDF cluster} \label{sec:processing}

The data processing was done using the Production and Distributed Analysis System (PanDA; \citeds{DMTN-168}) at the USDF at SLAC.
The PanDA system handled workflow orchestration and job retries.
Based on pre-production testing, five PanDA queues (\texttt{lowmem}, \texttt{mediummem}, \texttt{highmem}, \texttt{extra-highmem}, and  \texttt{merge}) were deployed. Each queue had fixed numbers of slots, memory limits, wallclock runtime limits and pull-or-push mode as described in this table:

The production processing was organized into seven logical ``steps''.
The high level workflow of workflows and the step organization is described in \citeds{RTN-001} Sect. 2.1.4.
Pipelines, V\&V, and Processing teams all focused on one step at a time.
Before the production processing started in each step, ``pilot runs'' with candidate software were carried out and signed off by V\&V and Pipelines teams
(Sect \ref{sec:management}).

The workflow generation and submission were done via the PanDA BPS plugin a terminal window logged into the USDF.
The BPS YAML configurations can be found in a branch of the  GitHub repo (https://github.com/lsst-dm/cm-prod).

Workflow progress was tracked via PanDA's iDDS monitoring page deployed at CERN and the JIRA-based campaign tooling (\citeds{RTN-023}).

On many occasions (20\%) rescue workflows skipping successful jobs were run.

The processing took place  between May 2023 to Sep 2023 and the total cpu usage over the course of PDR2 was approximately XXXX M core-hour; the compute resource usage is summarized below.
Notable issues which came up during the production were summarized as follows.

The Campain Management tools: $\rm cm\_prod$ and $\rm cm\_tools$ : \url{https://github.com/lsst-dm/cm_prod} were used to help manage this production.  

A summary of errors for all steps can be found at : \url{https://confluence.lsstcorp.org/pages/viewpage.action?pageId=222728293}

We comment here on some issues of note which arose during step1, 2 and 3 processing.

\begin{itemize}

\item Step1
\begin{itemize}

  \item
Step one was run using version v24.1.0.rc2 of the lsst software stack. We ran it in three distinct pieces, all with their own cm database.  WIDE: 14473 exposures (note exposure 428 (y-band) from the 2020 processing, left out accidently). DEEP: 1832 exposures. UDEEP: 1010 exposures (up to 426  exposures in one band covering a single tract on the sky: tract 9813, z-band).  

  \item
17315 exposures divided into 39 groups of typically 400 visits each for processing with PanDA/slurm workload/workflow/batch system on 3,000 cores at USDF.  Each group typically took 2.5 hours to generate the quantum graph + EXEC butler and had a run time of about 2 hours (not counting a long tail in many cases). Those
runs times are  when clustering of 5 pipetasks of step1 is used. Long tails, timeout, hangs from processing extended run time to a couple weeks wallclock for all of step1.

\item
Processing runs in the lowest memory PanDA queue (4GB/core) requestMemory allocation.

\item An inventory matching input (visit,detectors) to output calExps was performed.  Errors fell into a few groups, mostly common was insufficient stars
to determine a good psf.  There was some visits not processed 
in the narrow band filters due to missing input flat field
calibration files.  All were itemized on the Error Characterization page.

\end{itemize} %step1

\item Step2

\begin{itemize}

 \item
	 Step 2 is split into 2a (gather detector-based catalogs into
		visit catalogs), 2b (astrometric calibration), 2c (photometric
		calibration), 2d (apply calibrations) and 2e (gather
		calibrated visit catalogs) pieces.

\item 
	Each step2 component required more memory than originally
	assumed.  

	Step2a (gather catalogs) was run with v24.1.0.rc2 on
	WIDE, DEEP and UDEEP sets separately.  Following
	this, the software stack was switched to v24.1.0.rc3 to
	be ready for step2c which needed an ancillary map update to run on
	the whole of the HSC 710 tract footprint (rather than on a
	pair of test tracts).  At this time, the WIDE, DEEP and UDEEP
	collections were also merged into a single step2b output
	collection and the distinction between WIDE, DEEP and UDEEP
	was no longer reflected in the collection names.

	
	Step 2b required very long run times and increased memory 
	for the UDEEP patches which had up to N=436 of visits for
	some filter/tract combinations.

	Step 2c also required a change to the PanDA extrahimem queue
	to allow multi-core jobs to run (all other jobs are run 
	with 1 core/quanta).  It made it thru with 12 cores and 16 hours
	run time.

	The initial pipetaskInit memory footprint had to be increased from
	4GB to 16GB to enable reading of the step2c inputs. This
	was done in the requestMemory.yaml file.

	Step2e required a memory increase to 480GB to collect all the
	calibrated visit catalogs together. This is close to the 
	node maximum of 512GB -- so larger campaigns will need to adapt.
	There is a known issue where PSFs are read in unnecessiarily that
	can be corrected to improve the memory usage of step2e.

\item 
	After step2e, a 'visit veto' list was constructed to remove
	visits with known poor seeing.  After application of that 
	veto list, approximately 15K (out of the original 17.3K) visits
	remained for coadding.

\end{itemize} %step2

\item Step3
\begin{itemize}

  \item
	 MakeWarp was initially run 'clustered', however that increased memory
	usage unacceptably, so it was finally run unclusterd in the medium 
	queue.

  \item
	  The measure pipetask through errors which did not appear to be
	memory errors, however, increasing memory to 80GB and rerunning
	allowed this task to succeed.

  \item
	Running forcedPhotCoadd in the lowest memory queue resulted in
	100 copies to run on some nodes -- however this task generates a lot
	of scratch space, and the aggregate scratch space (3GB * 100 = 300 GB)
	was exceeded, resulting in failed jobs.
	To work around until more scratch space is available, we increased
	memory per job to 10GB, this then ran with < 50 jobs/node, and so
	didn't exceed 3*50 = 150 GB/node scratch space.

  \item
	  healSparsePropertyMaps failed on empty or partially empty tracts
	or patches.  A hotfix was made to the healSparse code by the photometric
	calibration scientist,  and using CM's custom setup ability, 
	the healSparse Maps were rerun successfully.

 \item
	 It was noted that many patches in deep coadds were empty of detected
	and measured objects. This was traced to the dynamic sky object finder
	settings (too few sky objects found in deep stacks).  This
	was fixed with a hot fix to meas algorithms and applied to patches
	in about 19 tracts successfully.

  \item
	 deblend pipetask had to be given extra memory in several instances 
	and rerun.

\end{itemize} %step3

\item Step4 -- 41 groups of 400 visits complete.  Quality analysis and
		collecting chains underway.  Ran in medium memory
		queue ($\rm 4 < MEM < 8 GB$).  No serious errors
		encountered.
	
\item Step5, Step6 -- in progress

\item Step7 -- complete Consolidation of HealSparsePropertyMaps for full PDR2 footprint.  This was done after the hotfix for HealSparse was applied (as in
	step3).

\end{itemize} %all steps

