\section{Campaign Management and Communication} \label{sec:management}

Here we cover the management structures in place for HSC PDR2 
this includes the groups and meetings like the change control for 
the pipeline version.

\subsection{Oversight}

Science Pipelines, DM Middleware, Campaign Management (CM), and Data Production
were all involved in HSC PDR2 production.

\subsection{Data Production}

For HSC PDR2, the active membership of the team was:
\begin{itemize}
\item Yusra AlSayyad -- Lead
\item Jennifer Adelman-McCarthy Pilot
\item Brian Yanny co-Pilot
\end{itemize}


\subsection{Campaign Management}
\begin{itemize}
\item Eric Charles -- Lead
\item Orion Eiger
\item Sierra Villarreal
\item Fritz Mueller
\end{itemize}

\subsection{System Performance}
\begin{itemize}
\item Colin Slater - Lead Verification and Validation Scientist
\end{itemize}

\subsection{Coordination}

The Data Production Lead defines campaigns (input datasets, software stack version, set of
steps (of pipetasks) to run, and rough deadline) to be run 
here \url{https://confluence.lsstcorp.org/display/DM/Campaigns}, and assigns pilots and co-pilots.
For the HSC PDR2 campaign, overall notes are kept here: \url{https://confluence.lsstcorp.org/display/DM/2023+Internal+HSC+PDR2+Reprocessing+at+the+USDF}.   As this dataset has been previously processed, it was useful
to reference the earlier reprocessing from 2020 to compare visit lists, tract lists and other notes:
\url{https://confluence.lsstcorp.org/display/DM/S20+HSC+PDR2+Reprocessing}.

During the production of HSC PDR2 weekly coordination meetings were held with the Data Production 
and V\&V leads, the pilots, PanDA experts, and Middleware and CM developer reps. 

\subsection{Work Management}

We used Jira to track work related to the HSC PDR2 campaign.
%Epics and milestones were created in the \texttt{DM} Jira Project.
The main ticket was \jira{DM-39132} with subtickets for each step.
These tickets contained processing notes, and special situations that came up.

The slack channel {\it\#ops-cm-team} was used for questions and discussion as issues arose.
The channel {\it\#dm-hsc-reprocessing} was also a valuable resource as weekly processing of small
sets of HSC data were discussed here.

\subsection{Change Control Decisions during processing}

The Science Pipelines team working with V\&V and middleware determined which weekly release to use as a base
software distribution for the v24 stack.  Some tickets subsequent to the initial v24 stack branching
were backported into the v24 branch.  PDR2 used v24.1.0.rc2 for steps 1, 2a and 2b.

During processing, the Science Pipelines team continued to monitor progress and determine if updates were
needed to the software while processing.

The stack was updated to v24.1.0.rc3 for step 2c and beyond to incorporate an updated fgcm (photometric
calibration map covering the whole HSC PDR2 footprint) \jira{DM-39342}.

During the Running of step3 and step7 two hot fixes were incorporated into processing, making use
of the CM '$\rm custom\_lsst\_setup$' feature, which allowed a github branch of code, along with a EUPS setup
file, to be loaded for processing on top of the base v24.1.0.rc3 stack.

The first hot fix was to the $\rm meas\_algorithms$ module to enable more robust dynamic sky estimation in very
crowded fields -- without this fix, large sections of the UDEEP tracts simply had no good detections or
measurements due to lack of sky objects.

The second hot fix was to the healSparse module to enable healSparsePropertyMaps (a step3 pipetask) and 
consolidateHealSparsePropertyMaps (step7) to proceed in cases where a tract or patch was not complete.
This allowed the NSC PDR2 production to proceed even for tracts which were not completely filled in.

We note that it was determined here that the stack and all hotfixes used to generate the quantum graph
needs to be identical to the stack used to process the quantum graph.  That is the setup used to make the graph
needs to have all the pieces used to execute the graph otherwise hotfix code patches will not work.

\subsection{Production Hardware and Workflow software system}

Panda Doma Queues, one may view the number of jobs currently running in each queue here:
\url{https://panda-doma.cern.ch/dash/region} (auth required, increase Show entries from 20 to 50)
A Pull mode queue means there is a pilot job running on a worker node which pulls jobs to it from PanDA and
can handle large numbers of jobs per pilot.  A Push mode queue means that there is only one pilot per
job (usually for very long running or high memory jobs). 

\normalsize 
\begin{center}
\begin{longtable}{|l|r|r|r|r|l|} 
\caption{PanDA Queue Names, Sizes, Time limits} \label{tab:pandaqueues}\\
\hline 
\textbf{Name}&\textbf{Slots}&\textbf{Memory}&\textbf{Wallclock}&\textbf{Push/Pull}&\textbf{Notes} \\ 
\hline
$\rm SLAC\_Rubin\_Merge$ & 200 & $<16$ GB & 24h & Push & MergeExecutionButler \\
$\rm SLAC\_Rubin$ & 4000 & $<4$ GB & 24h & Pull & 600 jobs/pilot \\
$\rm SLAC\_Rubin\_Medium$ & 3000 & $4-8$ GB & 24h & Pull &  600 jobs/pilot\\
$\rm SLAC\_Rubin\_Himem$ & 2500& $8-18$ GB & 24h & Pull &  600 jobs/pilot \\
$\rm SLAC\_Rubin\_Extra\_Himem$ & 1500 & $>18$ GB & 96h & Push & 1 job/pilot \\
\hline
\end{longtable} 
\end{center}
\normalsize

